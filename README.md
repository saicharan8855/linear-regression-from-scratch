\# Linear Regression from First Principles

\### \*A Complete Mathematical \& Empirical Reconstruction of the OLS Framework\*



<p align="center">

&nbsp; <img src="docs/geometry/least\_squares\_as\_projection.png" width="800" alt="Least Squares Projection"/>

</p>



<p align="center">

&nbsp; <i>This is not another high-level implementation. This repository contains a ground-up reconstruction of Linear Regression, moving from the probabilistic origins of Maximum Likelihood Estimation (MLE) to vectorized optimization and geometric projection.</i>

</p>



---



\## ğŸ¯ Why This Project Exists



Most implementations treat Linear Regression as a black box:

\- âŒ Import sklearn â†’ `fit()` â†’ `predict()` â†’ done

\- âŒ No understanding of \*why\* MSE is the optimal objective

\- âŒ No awareness of when assumptions break down

\- âŒ No appreciation for the underlying geometry



\*\*This project answers the questions others skip:\*\*

\- âœ… \*\*Why does minimizing MSE work?\*\* (It's maximum likelihood under Gaussian noise)

\- âœ… \*\*What is regression geometrically?\*\* (Orthogonal projection onto column space)

\- âœ… \*\*When does it fail?\*\* (Comprehensive diagnostics for all 7 classical assumptions)

\- âœ… \*\*How do we fix failures?\*\* (Ridge/Lasso regularization, proper validation)



This engine is built to be \*\*analytically tractable\*\* and \*\*statistically transparent\*\*. It bypasses high-level libraries like `scikit-learn` and `statsmodels` to expose the underlying matrix calculus.



---



\## ğŸ›ï¸ Project Architecture



```

linear-regression-from-scratch/

â”œâ”€â”€ ğŸ“š theory/                        # The "Why": Probabilistic \& Geometric derivations

â”‚   â”œâ”€â”€ MLE.ipynb                     # From likelihood to MSE

â”‚   â””â”€â”€ Geometry.ipynb                # Projection \& orthogonality proofs

â”‚

â”œâ”€â”€ ğŸ§® scripts/                       # The "How": Custom NumPy-based regression engine

â”‚   â”œâ”€â”€ linear\_regression.py          # Core OLS, Ridge, Lasso implementations

â”‚   â”œâ”€â”€ data\_preprocessing.py         # Feature engineering pipeline

â”‚   â””â”€â”€ model\_utils.py                # Metrics \& statistical inference

â”‚

â”œâ”€â”€ ğŸ““ notebooks/                     # The "Evidence": Systematic validation \& diagnostics

â”‚   â”œâ”€â”€ phase\_6\_diagnostics.ipynb     # Assumption testing

â”‚   â”œâ”€â”€ phase\_7\_model\_validation.ipynb  # Statistical inference

â”‚   â””â”€â”€ phase\_8\_final\_prediction.ipynb  # End-to-end pipeline

â”‚

â””â”€â”€ ğŸ“ docs/                          # The "Proof": Handwritten math \& visualizations

&nbsp;   â”œâ”€â”€ handwritten/                  # Scanned mathematical derivations

&nbsp;   â”‚   â”œâ”€â”€ MLE\_to\_MSE/              # Likelihood â†’ Cost function

&nbsp;   â”‚   â”œâ”€â”€ normal\_equation\_derivation/  # Matrix calculus proof

&nbsp;   â”‚   â”œâ”€â”€ ridge\_regression\_derivation/ # L2 penalty derivation

&nbsp;   â”‚   â””â”€â”€ noise\_distribution/       # Gaussian assumption

&nbsp;   â”œâ”€â”€ geometry/                     # Geometric visualizations

&nbsp;   â”‚   â”œâ”€â”€ least\_squares\_as\_projection.png

&nbsp;   â”‚   â”œâ”€â”€ gradient\_descent\_contour.png

&nbsp;   â”‚   â””â”€â”€ overfitting\_geometry.png

&nbsp;   â””â”€â”€ assumptions\_of\_LR/            # Diagnostic plots (10 images)

```



---



\## ğŸ§  Core Research Pillars



\### 1. Probabilistic Origin: MLE â†’ MSE



<p align="center">

&nbsp; <img src="docs/handwritten/noise\_distribution/gaussian\_noise\_visualization.png" width="700" alt="Gaussian Noise Distribution"/>

</p>



We treat the target $y$ as a random variable sampled from a Gaussian distribution:



$$y = X\\theta + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$$



By maximizing the Log-Likelihood $\\mathcal{L}(\\theta)$, we prove that \*\*minimizing Mean Squared Error (MSE) is not an arbitrary choice\*\*â€”it is the statistically optimal objective under the assumption of Gaussian noise.



\*\*The Key Insight:\*\*

```

Maximizing P(y|X,Î¸)  âŸº  Minimizing Î£(y - XÎ¸)Â²

```



ğŸ“– \*\*Full Derivation:\*\* \[`theory/MLE.ipynb`](theory/MLE.ipynb)  

ğŸ“ \*\*Handwritten Proof:\*\* \[`docs/handwritten/MLE\_to\_MSE/`](docs/handwritten/MLE\_to\_MSE/)



---



\### 2. The Geometry of Orthogonality



<p align="center">

&nbsp; <img src="docs/geometry/least\_squares\_as\_projection.png" width="800" alt="Geometric Interpretation"/>

</p>



Linear Regression is fundamentally a \*\*projection problem\*\*. We find the vector $\\hat{y}$ in the Column Space of $X$ that is closest to the observed $y$. This occurs when the residual vector $e = y - \\hat{y}$ is \*\*orthogonal\*\* to every column of $X$.



$$X^T(y - X\\theta) = 0 \\quad \\implies \\quad \\theta = (X^T X)^{-1} X^T y$$



\*\*Geometric Interpretation:\*\*

\- The blue plane = Column space of $X$ (all possible predictions)

\- Red vector = Observed $y$ (target data)

\- Green vector = $\\hat{y}$ (optimal prediction via orthogonal projection)

\- Orange vector = Residual $e$ (perpendicular to the column space)



\*\*Key Property:\*\* The residual is orthogonal to predictions â†’ $\\hat{y}^T e = 0$



ğŸ“– \*\*Deep Dive:\*\* \[`theory/Geometry.ipynb`](theory/Geometry.ipynb)  

ğŸ“ \*\*Handwritten Notes:\*\* \[`docs/handwritten/normal\_equation\_derivation/`](docs/handwritten/normal\_equation\_derivation/)



---



\### 3. Dual Optimization Paths



<p align="center">

&nbsp; <img src="docs/geometry/gradient\_descent\_contour.png" width="800" alt="Gradient Descent Convergence"/>

</p>



I have implemented two distinct methods for finding the optimal parameter vector $\\theta$:



\#### \*\*Method 1: The Normal Equation (Closed-Form)\*\*

```python

Î¸ = (X^T X)^(-1) X^T y

```

\- âœ… Exact analytical solution in one step

\- âœ… No hyperparameter tuning required

\- âŒ O(nÂ³) complexity â€” expensive for large datasets

\- âŒ Fails when $X^T X$ is singular (multicollinearity)



\#### \*\*Method 2: Vectorized Gradient Descent (Iterative)\*\*

```python

Î¸ := Î¸ - Î± Â· (1/m) Â· X^T(XÎ¸ - y)

```

\- âœ… Scalable to massive datasets

\- âœ… Works even when $X^T X$ is near-singular

\- âŒ Requires careful tuning of learning rate $\\alpha$

\- âŒ Needs feature scaling for efficient convergence



\*\*Visualization Insights:\*\*

\- \*\*Left Panel:\*\* 3D cost surface showing gradient descent path

\- \*\*Middle Panel:\*\* Contour plot comparing learning rates (Î± = 0.01, 0.1, 0.5)

\- \*\*Right Panel:\*\* Convergence curves showing cost vs iterations



\*\*Key Finding:\*\* Optimal learning rate achieves convergence in ~120 epochs with proper feature scaling.



---



\## ğŸ›¡ï¸ Regularization \& Numerical Stability



To handle the high-dimensional \*\*Ames Housing Dataset\*\*, the engine implements L2 (Ridge) and L1 (Lasso) penalties.



\### Ridge Regression (L2 Penalty)



<p align="center">

&nbsp; <img src="docs/handwritten/ridge\_regression\_derivation/ridge\_regression\_derivation(1).jpg" width="600" alt="Ridge Derivation"/>

</p>



\*\*The Problem:\*\* When features are correlated, $X^T X$ becomes near-singular â†’ unstable parameter estimates



\*\*The Solution:\*\* Add regularization term to stabilize matrix inversion:



$$\\theta\_{ridge} = (X^T X + \\lambda I)^{-1} X^T y$$



\*\*Effect:\*\*

\- Shrinks coefficients toward zero

\- Reduces variance at the cost of slight bias

\- Solves multicollinearity issues

\- \*\*Note:\*\* Bias term explicitly excluded from penalty



ğŸ“ \*\*Handwritten Derivation:\*\* \[`docs/handwritten/ridge\_regression\_derivation/`](docs/handwritten/ridge\_regression\_derivation/)



\### Lasso Regression (L1 Penalty)



\*\*The Difference:\*\* L1 penalty creates \*\*sparse solutions\*\* (sets some coefficients to exactly zero)



\*\*Implementation:\*\* Coordinate descent algorithm (no closed-form solution exists)



\*\*Use Case:\*\* Automatic feature selection â€” eliminates irrelevant features



\*\*Weight Path Analysis:\*\* Visualizes how coefficients shrink to zero as $\\lambda$ increases



---



\## ğŸš¨ Stress-Testing \& Failure Analysis



> \*"The best modelers know when their models are lying."\*



This project includes a comprehensive \*\*Diagnostic Suite\*\* to verify the classical OLS assumptions:



| Assumption | What It Means | Diagnostic Tool | Status | Violation Impact |

|------------|---------------|-----------------|--------|------------------|

| \*\*Linearity\*\* | Relationship is actually linear | Residual vs Fitted Plot | âœ… Verified | Systematic bias in predictions |

| \*\*Independence\*\* | Errors are uncorrelated | Durbin-Watson Analysis | âœ… Verified | Underestimated standard errors |

| \*\*Homoscedasticity\*\* | Constant error variance | Residual spread analysis | âœ… Verified | Invalid confidence intervals |

| \*\*Normality of Errors\*\* | Residuals follow Gaussian | Q-Q Plot \& Histogram | âœ… Verified | Poor statistical inference |

| \*\*No Multicollinearity\*\* | Features aren't redundant | Correlation Heatmaps | âœ… Verified | Unstable, uninterpretable coefficients |

| \*\*No Outliers\*\* | Extreme values don't dominate | Cook's Distance | âœ… Verified | Biased parameter estimates |

| \*\*Mean-Zero Errors\*\* | No systematic bias | Residual histogram | âœ… Verified | Model misspecification |



<p align="center">

&nbsp; <img src="docs/assumptions\_of\_LR/homoscedasticity.png" width="350" alt="Homoscedasticity Test"/>

&nbsp; <img src="docs/assumptions\_of\_LR/normality\_errors.png" width="350" alt="Normality Test"/>

</p>



\*\*All 10 diagnostic plots:\*\* \[`docs/assumptions\_of\_LR/`](docs/assumptions\_of\_LR/)



---



\## ğŸ“Š Implementation Details



\### The Regression Engine (`scripts/linear\_regression.py`)



```python

class LinearRegressionMaster:

&nbsp;   """

&nbsp;   A from-scratch implementation of Linear Regression with:

&nbsp;   - OLS (Normal Equation)

&nbsp;   - Gradient Descent

&nbsp;   - Ridge \& Lasso Regularization

&nbsp;   - Statistical Inference (t-tests, p-values)

&nbsp;   """

&nbsp;   

&nbsp;   def fit\_ols(self, X, y):

&nbsp;       """Closed-form solution via Normal Equation"""

&nbsp;       self.theta = np.linalg.inv(X.T @ X) @ X.T @ y

&nbsp;   

&nbsp;   def fit\_gradient\_descent(self, X, y, learning\_rate=0.01, epochs=1000):

&nbsp;       """Iterative optimization with vectorized gradient"""

&nbsp;       m = len(y)

&nbsp;       for \_ in range(epochs):

&nbsp;           gradient = (1/m) \* X.T @ (X @ self.theta - y)

&nbsp;           self.theta -= learning\_rate \* gradient

&nbsp;   

&nbsp;   def fit\_ridge(self, X, y, lambda\_reg=1.0):

&nbsp;       """Ridge Regression (L2 penalty)"""

&nbsp;       n\_features = X.shape\[1]

&nbsp;       I = np.eye(n\_features)

&nbsp;       I\[0, 0] = 0  # Don't penalize bias term

&nbsp;       self.theta = np.linalg.inv(X.T @ X + lambda\_reg \* I) @ X.T @ y

&nbsp;   

&nbsp;   def predict(self, X):

&nbsp;       """Vectorized predictions"""

&nbsp;       return X @ self.theta

&nbsp;   

&nbsp;   def compute\_standard\_errors(self, X, y):

&nbsp;       """Statistical inference: Var(Î¸) = ÏƒÂ² (X^T X)^(-1)"""

&nbsp;       residuals = y - self.predict(X)

&nbsp;       dof = len(y) - len(self.theta)

&nbsp;       sigma\_squared = np.sum(residuals\*\*2) / dof

&nbsp;       var\_theta = sigma\_squared \* np.linalg.inv(X.T @ X)

&nbsp;       return np.sqrt(np.diag(var\_theta))

&nbsp;   

&nbsp;   def compute\_t\_statistics(self, X, y):

&nbsp;       """Hypothesis testing for coefficient significance"""

&nbsp;       se = self.compute\_standard\_errors(X, y)

&nbsp;       return self.theta / se

&nbsp;   

&nbsp;   def compute\_p\_values(self, X, y):

&nbsp;       """Two-tailed p-values from t-distribution"""

&nbsp;       from scipy import stats

&nbsp;       t\_stats = self.compute\_t\_statistics(X, y)

&nbsp;       dof = len(y) - len(self.theta)

&nbsp;       return 2 \* (1 - stats.t.cdf(np.abs(t\_stats), dof))

```



\*\*What's Implemented:\*\*

\- âœ… OLS (Normal Equation)

\- âœ… Gradient Descent with convergence tracking

\- âœ… Ridge Regression (closed-form)

\- âœ… Lasso Regression (coordinate descent)

\- âœ… Standard errors, t-statistics, p-values

\- âœ… RÂ², Adjusted RÂ², RMSE

\- âœ… Prediction intervals



\*\*What's NOT Used:\*\*

\- âŒ `sklearn.linear\_model.LinearRegression`

\- âŒ `statsmodels.api.OLS`

\- âŒ Any pre-built statistical solver



---



\## ğŸ”¬ Experimental Results



\### Overfitting Analysis



<p align="center">

&nbsp; <img src="docs/geometry/overfitting\_geometry.png" width="800" alt="Bias-Variance Tradeoff"/>

</p>



\*\*Key Findings:\*\*

\- \*\*Degree 1 (Underfitting):\*\* High bias, cannot capture true patterns

\- \*\*Degree 3 (Optimal):\*\* Balanced bias-variance, generalizes well

\- \*\*Degree 15 (Overfitting):\*\* Zero training error, terrible test performance



\*\*Lesson:\*\* Model complexity must balance bias (underfitting) and variance (overfitting)



---



\### Performance on Ames Housing Dataset



| Metric | Train | Test | Insight |

|--------|-------|------|---------|

| \*\*RÂ²\*\* | 0.87 | 0.82 | Captured 82% of variance in housing prices |

| \*\*RMSE\*\* | $18,500 | $21,200 | Average prediction error reasonable for price range |

| \*\*Adjusted RÂ²\*\* | 0.86 | 0.81 | Penalizes complexity, still strong |

| \*\*Convergence\*\* | 120 Epochs | â€” | Achieved with feature scaling (Z-score normalization) |



\*\*Note:\*\* These metrics validate the implementation, but \*\*understanding the failures\*\* is more valuable than the scores.



---



\### Feature Scaling Impact



\*\*Without Scaling:\*\*

\- Gradient descent oscillates wildly

\- Requires 10,000+ iterations to converge

\- Learning rate must be microscopic (Î± < 0.00001)

\- Cost surface becomes elongated ellipse



\*\*With Scaling:\*\*

\- Smooth convergence in ~100-120 iterations

\- Stable across wider range of learning rates

\- Cost surface becomes nearly spherical

\- Gradient points directly toward minimum



\*\*Conclusion:\*\* Feature scaling (standardization) is \*\*mandatory\*\* for efficient gradient descent.



---



\## ğŸ¯ Dataset: Ames Housing Prices



\*\*Why this dataset?\*\*

\- 89 features (numerical + categorical mix)

\- Real missing values requiring intelligent imputation

\- Natural outliers (luxury homes, foreclosures)

\- Clear violations of homoscedasticity (price heterogeneity)

\- Target requires log-transformation (right-skewed distribution)



\*\*Preprocessing Pipeline:\*\*

1\. \*\*Log-transform target:\*\* `SalePrice` â†’ `log(SalePrice)` (stabilizes variance)

2\. \*\*Handle missing values:\*\* Median for numerical, "None" for categorical

3\. \*\*Feature selection:\*\* 10-15 most predictive features via correlation analysis

4\. \*\*One-hot encoding:\*\* Manual implementation (no sklearn `get\_dummies`)

5\. \*\*Add bias term:\*\* Explicitly prepend column of ones

6\. \*\*Standardize features:\*\* Z-score normalization using \*\*training statistics only\*\*

7\. \*\*Train/test split:\*\* 80/20 ratio, implemented from scratch



\*\*Critical Detail:\*\* Standardization uses training set mean/std to prevent data leakage into test set.



ğŸ“„ \*\*Full Pipeline:\*\* \[`scripts/data\_preprocessing.py`](scripts/data\_preprocessing.py)



---



\## ğŸš€ How to Execute



\### 1. Clone the Repository

```bash

git clone https://github.com/saicharan8855/linear-regression-from-scratch.git

cd linear-regression-from-scratch

```



\### 2. Install Dependencies

```bash

pip install -r requirements.txt

```



\*\*Required packages:\*\*

\- `numpy` â€” Matrix operations

\- `pandas` â€” Data manipulation

\- `matplotlib` â€” Visualizations

\- `scipy` â€” Statistical distributions (t-test, p-values)



\### 3. Explore the Theory

```bash

jupyter notebook theory/MLE.ipynb

jupyter notebook theory/Geometry.ipynb

```



\### 4. Run Diagnostics \& Validation

```bash

jupyter notebook notebooks/phase\_6\_diagnostics.ipynb      # Assumption testing

jupyter notebook notebooks/phase\_7\_model\_validation.ipynb  # Statistical inference

```



\### 5. Generate Final Predictions

```bash

jupyter notebook notebooks/phase\_8\_final\_prediction.ipynb

```



\*\*Output:\*\* `outputs/predicted\_sale\_prices.csv` â€” Ready for Kaggle submission



---



\## ğŸ“š What I Learned



\### Technical Skills Developed

\- âœ… \*\*Deriving estimators from probability theory\*\* (MLE â†’ MSE proof)

\- âœ… \*\*Implementing matrix operations without libraries\*\* (pure NumPy)

\- âœ… \*\*Debugging gradient descent\*\* (learning rates, scaling, convergence diagnostics)

\- âœ… \*\*Statistical inference from scratch\*\* (t-tests, p-values, confidence intervals)

\- âœ… \*\*Recognizing when theory breaks down\*\* (assumption violations, diagnostics)



\### Conceptual Insights Gained

\- \*\*Linear Regression is geometry:\*\* It projects data onto subspaces

\- \*\*OLS is probabilistic:\*\* It's maximum likelihood under Gaussian noise assumption

\- \*\*Regularization is necessary:\*\* Real data always violates independence assumptions

\- \*\*Metrics can mislead:\*\* High RÂ² â‰  valid assumptions

\- \*\*Feature engineering > model complexity:\*\* 10 good features beat 100 mediocre ones

\- \*\*Scaling is mandatory:\*\* Gradient descent fails catastrophically without it



\### Why This Matters

Understanding \*why\* algorithms work makes you a \*\*10x better debugger\*\*. When a model fails, you know where to look. When assumptions break, you know how to fix them. This is the difference between "can use sklearn" and "understands machine learning."



---



\## ğŸ”® Future Extensions



\- \[ ] \*\*Weighted Least Squares\*\* â€” Handle heteroscedasticity explicitly

\- \[ ] \*\*Generalized Linear Models\*\* â€” Extend to logistic, Poisson regression

\- \[ ] \*\*Bayesian Linear Regression\*\* â€” Posterior distributions over parameters

\- \[ ] \*\*Robust Regression\*\* â€” Outlier-resistant estimators (Huber loss)

\- \[ ] \*\*Time Series Extensions\*\* â€” Handle autocorrelation (AR, ARMA models)

\- \[ ] \*\*GPU Acceleration\*\* â€” CuPy implementation for massive datasets



---



\## ğŸ“– References



\*\*Foundational Texts:\*\*

\- Hastie, Tibshirani, Friedman â€” \*The Elements of Statistical Learning\*

\- Christopher Bishop â€” \*Pattern Recognition and Machine Learning\*

\- Gilbert Strang â€” MIT 18.065: \*Matrix Methods in Data Analysis\*



\*\*Dataset Source:\*\*

\- Dean De Cock (2011) â€” \[Ames Housing Dataset](http://jse.amstat.org/v19n3/decock.pdf)



\*\*Inspirations:\*\*

\- Andrew Ng â€” Stanford CS229 Lecture Notes

\- StatQuest â€” Josh Starmer's YouTube series on regression



---



\## ğŸ“§ Connect With Me



<p align="center">

&nbsp; <a href="mailto:saicharan9948644390@gmail.com">ğŸ“§ Email</a> â€¢

&nbsp; <a href="https://www.linkedin.com/in/saicharan-k-a7b5a5267/">ğŸ’¼ LinkedIn</a> â€¢

&nbsp; <a href="https://github.com/saicharan8855">ğŸ™ GitHub</a>

</p>



---



\## â­ If This Helped You



If you found this repository useful for understanding Linear Regression deeply:

\- â­ \*\*Star the repo\*\* â€” Helps others discover it

\- ğŸ´ \*\*Fork it\*\* â€” Build your own experiments

\- ğŸ“¢ \*\*Share it\*\* â€” Help others learn from first principles



---



<p align="center">

&nbsp; <img src="https://img.shields.io/github/stars/saicharan8855/linear-regression-from-scratch?style=social" alt="GitHub stars"/>

&nbsp; <img src="https://img.shields.io/github/forks/saicharan8855/linear-regression-from-scratch?style=social" alt="GitHub forks"/>

</p>



<p align="center">

&nbsp; <i>"The best way to understand an algorithm is to rebuild it from the atoms up."</i>

</p>



<p align="center">

&nbsp; <b>Built with theory, implemented from scratch, tested exhaustively.</b>

</p>



<p align="center">

&nbsp; <i>This project is that philosophy in action.</i>

</p>



---



<p align="center">

&nbsp; <sub>Â© 2025 Sai Charan. This project is MIT licensed.</sub>

</p>

