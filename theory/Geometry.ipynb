{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0801d-3cf6-49e6-8897-11b35fef8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"This section presents a geometric interpretation of linear regression, showing that the optimal prediction vector is obtained by projecting the target vector $y$ onto the column space of the design matrix $X$. This viewpoint explains the normal equation as a consequence of orthogonality between the residual vector and the feature space.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Vector Space Setup\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Column Space of the Design Matrix\\n\",\n",
    "    \"\\n\",\n",
    "    \"The column space of the design matrix $X$, denoted as $\\\\text{Col}(X)$, is the subspace of $\\\\mathbb{R}^m$ spanned by the columns of $X$. Any vector in $\\\\text{Col}(X)$ can be written as a linear combination of the columns of $X$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For any parameter vector $\\\\theta$, the predicted response  \\n\",\n",
    "    \"$$\\\\hat{y} = X\\\\theta$$  \\n\",\n",
    "    \"lies in the column space of $X$. Therefore, linear regression restricts predictions to vectors that belong to $\\\\text{Col}(X)$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Projection of the Target Vector\\n\",\n",
    "    \"\\n\",\n",
    "    \"In general, the observed target vector $y$ does not lie in the column space of $X$. Linear regression therefore seeks a vector $\\\\hat{y} \\\\in \\\\text{Col}(X)$ that is closest to $y$ in the Euclidean sense.\\n\",\n",
    "    \"\\n\",\n",
    "    \"This closest vector is obtained by orthogonally projecting $y$ onto the column space of $X$. The resulting vector $\\\\hat{y} = X\\\\theta$ represents the best linear approximation to $y$ within $\\\\text{Col}(X)$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Orthogonality of the Residual\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let the residual vector be defined as  \\n\",\n",
    "    \"$$r = y - \\\\hat{y} = y - X\\\\theta.$$  \\n\",\n",
    "    \"A fundamental property of orthogonal projection is that the residual vector is orthogonal to the subspace onto which the projection is made. Therefore, the residual $r$ is orthogonal to the column space of $X$:  \\n\",\n",
    "    \"$$X^\\\\top r = 0.$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Derivation of the Normal Equation\\n\",\n",
    "    \"\\n\",\n",
    "    \"From the orthogonality condition  \\n\",\n",
    "    \"$$X^\\\\top r = 0,$$  \\n\",\n",
    "    \"and using the definition of the residual $r = y - X\\\\theta$, we obtain  \\n\",\n",
    "    \"$$X^\\\\top (y - X\\\\theta) = 0.$$  \\n\",\n",
    "    \"Expanding the expression gives  \\n\",\n",
    "    \"$$X^\\\\top y - X^\\\\top X\\\\theta = 0.$$  \\n\",\n",
    "    \"Rearranging terms yields  \\n\",\n",
    "    \"$$X^\\\\top X\\\\theta = X^\\\\top y.$$  \\n\",\n",
    "    \"If $X^\\\\top X$ is invertible, the solution for $\\\\theta$ is  \\n\",\n",
    "    \"$$\\\\theta = (X^\\\\top X)^{-1} X^\\\\top y.$$\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Geometric Interpretation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Geometrically, linear regression finds the vector $\\\\hat{y} = X\\\\theta$ in the column space of $X$ that is closest to the target vector $y$. The normal equation ensures that the residual vector $y - \\\\hat{y}$ is orthogonal to every column of $X$, meaning no further movement within the column space can reduce the distance to $y$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Thus, the normal equation represents the condition for an orthogonal projection of $y$ onto $\\\\text{Col}(X)$.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Conclusion\\n\",\n",
    "    \"\\n\",\n",
    "    \"From a geometric perspective, linear regression is the problem of projecting the target vector $y$ onto the column space of the design matrix $X$. The normal equation arises naturally from the orthogonality condition of this projection, providing a closedâ€‘form solution for the optimal parameters.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
