{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5355a31",
   "metadata": {},
   "source": [
    "This section presents a geometric interpretation of linear regression, showing that the optimal prediction vector is obtained by projecting the target vector $y$ onto the column space of the design matrix $X$. This viewpoint explains the normal equation as a consequence of orthogonality between the residual vector and the feature space.\n",
    "\n",
    "## Vector Space Setup\n",
    "\n",
    "## Column Space of the Design Matrix\n",
    "\n",
    "The column space of the design matrix $X$, denoted as $\\text{Col}(X)$, is the subspace of $\\mathbb{R}^m$ spanned by the columns of $X$. Any vector in $\\text{Col}(X)$ can be written as a linear combination of the columns of $X$.\n",
    "\n",
    "For any parameter vector $\\theta$, the predicted response  \n",
    "$$\\hat{y} = X\\theta$$  \n",
    "lies in the column space of $X$. Therefore, linear regression restricts predictions to vectors that belong to $\\text{Col}(X)$.\n",
    "\n",
    "## Projection of the Target Vector\n",
    "\n",
    "In general, the observed target vector $y$ does not lie in the column space of $X$. Linear regression therefore seeks a vector $\\hat{y} \\in \\text{Col}(X)$ that is closest to $y$ in the Euclidean sense.\n",
    "\n",
    "This closest vector is obtained by orthogonally projecting $y$ onto the column space of $X$. The resulting vector $\\hat{y} = X\\theta$ represents the best linear approximation to $y$ within $\\text{Col}(X)$.\n",
    "\n",
    "## Orthogonality of the Residual\n",
    "\n",
    "Let the residual vector be defined as  \n",
    "$$r = y - \\hat{y} = y - X\\theta.$$  \n",
    "A fundamental property of orthogonal projection is that the residual vector is orthogonal to the subspace onto which the projection is made. Therefore, the residual $r$ is orthogonal to the column space of $X$:  \n",
    "$$X^\\top r = 0.$$\n",
    "\n",
    "## Derivation of the Normal Equation\n",
    "\n",
    "From the orthogonality condition  \n",
    "$$X^\\top r = 0,$$  \n",
    "and using the definition of the residual $r = y - X\\theta$, we obtain  \n",
    "$$X^\\top (y - X\\theta) = 0.$$  \n",
    "Expanding the expression gives  \n",
    "$$X^\\top y - X^\\top X\\theta = 0.$$  \n",
    "Rearranging terms yields  \n",
    "$$X^\\top X\\theta = X^\\top y.$$  \n",
    "If $X^\\top X$ is invertible, the solution for $\\theta$ is  \n",
    "$$\\theta = (X^\\top X)^{-1} X^\\top y.$$\n",
    "\n",
    "## Geometric Interpretation\n",
    "\n",
    "Geometrically, linear regression finds the vector $\\hat{y} = X\\theta$ in the column space of $X$ that is closest to the target vector $y$. The normal equation ensures that the residual vector $y - \\hat{y}$ is orthogonal to every column of $X$, meaning no further movement within the column space can reduce the distance to $y$.\n",
    "\n",
    "Thus, the normal equation represents the condition for an orthogonal projection of $y$ onto $\\text{Col}(X)$.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "From a geometric perspective, linear regression is the problem of projecting the target vector $y$ onto the column space of the design matrix $X$. The normal equation arises naturally from the orthogonality condition of this projection, providing a closedâ€‘form solution for the optimal parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
